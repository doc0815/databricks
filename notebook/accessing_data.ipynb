{"cells":[{"cell_type":"markdown","source":["# DataBricks File System (DBFS)\nSmall files can be uploaded from the local machine into the DataBricks file system by dropping them in the 'Import & Explore Data' box on the landing page."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\ndata1 = pd.read_csv('/dbfs/FileStore/tables/data1.csv')\ndata2 = pd.read_json('/dbfs/FileStore/tables/data2.json')\nsparkMutualFundsDF = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/mutual_funds.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["print(data1.info())\nprint('\\n')\nprint(data1.describe())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nRangeIndex: 25000 entries, 0 to 24999\nData columns (total 6 columns):\nid         25000 non-null object\nsla        25000 non-null float64\noverdue    25000 non-null float64\nx          25000 non-null float64\ny          25000 non-null float64\ntype       25000 non-null float64\ndtypes: float64(5), object(1)\nmemory usage: 1.1+ MB\nNone\n\n\n                sla       overdue             x             y          type\ncount  25000.000000  25000.000000  2.500000e+04  2.500000e+04  25000.000000\nmean      30.992280     -4.798978  3.109990e+06  1.383583e+07      2.945040\nstd       51.215171    102.036356  3.451559e+04  3.273077e+04      0.266725\nmin        1.000000   -501.040000  2.077445e+06  1.370344e+07      0.000000\n25%        6.000000    -14.950000  3.085637e+06  1.381152e+07      3.000000\n50%       14.000000     -6.100000  3.111065e+06  1.383432e+07      3.000000\n75%       25.000000     -1.050000  3.133117e+06  1.385723e+07      3.000000\nmax      180.000000   1399.120000  3.207514e+06  1.396255e+07      5.000000\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["display(dbutils.fs.ls('/FileStore/tables'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/data1.csv</td><td>data1.csv</td><td>1438285</td></tr><tr><td>dbfs:/FileStore/tables/data2.json</td><td>data2.json</td><td>2768126</td></tr><tr><td>dbfs:/FileStore/tables/mutual_funds.csv</td><td>mutual_funds.csv</td><td>16795949</td></tr></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"code","source":["%fs\nls dbfs:/databricks-datasets/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["dbutils.fs.ls('dbfs:/FileStore/tables/')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">13</span><span class=\"ansired\">]: </span>\n[FileInfo(path=&apos;dbfs:/FileStore/tables/data1.csv&apos;, name=&apos;data1.csv&apos;, size=1438285),\n FileInfo(path=&apos;dbfs:/FileStore/tables/data2.json&apos;, name=&apos;data2.json&apos;, size=2768126),\n FileInfo(path=&apos;dbfs:/FileStore/tables/mutual_funds.csv&apos;, name=&apos;mutual_funds.csv&apos;, size=16795949)]\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["# Databases and Tables"],"metadata":{}},{"cell_type":"markdown","source":["## DBFS\nThe files that have been uploaded to DataBrick's filesystem can be converted into tables."],"metadata":{}},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS diamonds;\n\nCREATE TABLE diamonds\nUSING csv\nOPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"code","source":["%sql\nselect  *\nfrom    diamonds\nwhere   carat > 1.0\nlimit   5\n;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>depth</th><th>table</th><th>price</th><th>x</th><th>y</th><th>z</th></tr></thead><tbody><tr><td>173</td><td>1.17</td><td>Very Good</td><td>J</td><td>I1</td><td>60.2</td><td>61</td><td>2774</td><td>6.83</td><td>6.9</td><td>4.13</td></tr><tr><td>216</td><td>1.01</td><td>Premium</td><td>F</td><td>I1</td><td>61.8</td><td>60</td><td>2781</td><td>6.39</td><td>6.36</td><td>3.94</td></tr><tr><td>242</td><td>1.01</td><td>Fair</td><td>E</td><td>I1</td><td>64.5</td><td>58</td><td>2788</td><td>6.29</td><td>6.21</td><td>4.03</td></tr><tr><td>243</td><td>1.01</td><td>Premium</td><td>H</td><td>SI2</td><td>62.7</td><td>59</td><td>2788</td><td>6.31</td><td>6.22</td><td>3.93</td></tr><tr><td>248</td><td>1.05</td><td>Very Good</td><td>J</td><td>SI2</td><td>63.2</td><td>56</td><td>2789</td><td>6.49</td><td>6.45</td><td>4.09</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["The file `data1.csv` has also been converted into a table. You can analyse the data using `SQL` or `Python` on the pandas dataframe. Alternatively, you can use ordinary Unix pipeline processing using the `shell` (as this file is very small)."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect  id\n      , overdue\n      , type\nfrom    data1\nwhere   sla = 10\nand     overdue between -5.05 and -5.0\norder by\n        id\n;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>overdue</th><th>type</th></tr></thead><tbody><tr><td>101000963935</td><td>-5.05</td><td>3.0</td></tr><tr><td>101001006294</td><td>-5.0</td><td>3.0</td></tr><tr><td>101001007454</td><td>-5.04</td><td>3.0</td></tr><tr><td>101001024662</td><td>-5.0</td><td>3.0</td></tr><tr><td>101001032029</td><td>-5.05</td><td>3.0</td></tr><tr><td>101001060956</td><td>-5.04</td><td>3.0</td></tr><tr><td>101001117300</td><td>-5.05</td><td>3.0</td></tr><tr><td>101001141334</td><td>-5.05</td><td>3.0</td></tr><tr><td>101001141402</td><td>-5.02</td><td>3.0</td></tr><tr><td>101001155113</td><td>-5.05</td><td>3.0</td></tr><tr><td>101001168351</td><td>-5.02</td><td>3.0</td></tr><tr><td>101001201815</td><td>-5.01</td><td>3.0</td></tr><tr><td>101001201944</td><td>-5.05</td><td>3.0</td></tr><tr><td>101001213356</td><td>-5.01</td><td>3.0</td></tr><tr><td>11615148-101000944303</td><td>-5.02</td><td>3.0</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"code","source":["data1.loc[(data1['sla']==10) & (data1['overdue']>=-5.05) & (data1['overdue']<=-5.0),['id','overdue','type']].sort_values(by='id')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">43</span><span class=\"ansired\">]: </span>\n                          id  overdue  type\n8321            101000963935    -5.05   3.0\n16479           101001006294    -5.00   3.0\n17917           101001007454    -5.04   3.0\n4821            101001024662    -5.00   3.0\n13688           101001032029    -5.05   3.0\n3681            101001060956    -5.04   3.0\n19022           101001117300    -5.05   3.0\n7569            101001141334    -5.05   3.0\n2748            101001141402    -5.02   3.0\n20860           101001155113    -5.05   3.0\n5627            101001168351    -5.02   3.0\n1157            101001201815    -5.01   3.0\n14443           101001201944    -5.05   3.0\n23835           101001213356    -5.01   3.0\n8862   11615148-101000944303    -5.02   3.0\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["%sh\ncut -d ',' -f 1,2,3,6 0< '/dbfs/FileStore/tables/data1.csv' | grep ',10.0,' | cut -d ',' -f 1,3,4 | grep -E ',-5\\.0[^6-9]' | sort -k 1,1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">101000963935,-5.05,3.0\n101001006294,-5.0,3.0\n101001007454,-5.04,3.0\n101001024662,-5.0,3.0\n101001032029,-5.05,3.0\n101001060956,-5.04,3.0\n101001117300,-5.05,3.0\n101001141334,-5.05,3.0\n101001141402,-5.02,3.0\n101001155113,-5.05,3.0\n101001168351,-5.02,3.0\n101001201815,-5.01,3.0\n101001201944,-5.05,3.0\n101001213356,-5.01,3.0\n11615148-101000944303,-5.02,3.0\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Convert the Spark dataframe into a table"],"metadata":{}},{"cell_type":"code","source":["sparkMutualFundsDF.write.saveAsTable('mutual_funds')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["%sql\nselect * from mutual_funds limit 5;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fund_name</th><th>fund_extended_name</th><th>category</th><th>fund_family</th><th>net_assets</th><th>ytd_return</th><th>fund_yield</th><th>morningstar_rating</th><th>inception_date</th><th>investment</th><th>size</th><th>currency</th><th>net_annual_expense_ratio_fund</th><th>net_annual_expense_ratio_category</th><th>portfolio_cash</th><th>portfolio_stocks</th><th>portfolio_bonds</th><th>portfolio_others</th><th>portfolio_preferred</th><th>portfolio_convertable</th><th>price_earnings</th><th>price_book</th><th>price_sales</th><th>price_cashflow</th><th>median_market_cap</th><th>basic_materials</th><th>consumer_cyclical</th><th>financial_services</th><th>real_estate</th><th>consumer_defensive</th><th>healthcare</th><th>utilities</th><th>communication_services</th><th>energy</th><th>industrials</th><th>technology</th><th>bond_maturity</th><th>bond_duration</th><th>rating_us_government</th><th>rating_aaa</th><th>rating_aa</th><th>rating_a</th><th>rating_bbb</th><th>rating_bb</th><th>rating_b</th><th>rating_below_b</th><th>rating_others</th><th>morningstar_return_rating</th><th>fund_return_ytd</th><th>category_return_ytd</th><th>fund_return_1month</th><th>category_return_1month</th><th>fund_return_3months</th><th>category_return_3months</th><th>fund_return_1year</th><th>category_return_1year</th><th>fund_return_3years</th><th>category_return_3years</th><th>fund_return_5years</th><th>category_return_5years</th><th>fund_return_10years</th><th>category_return_10years</th><th>fund_return_2018</th><th>category_return_2018</th><th>fund_return_2017</th><th>category_return_2017</th><th>fund_return_2016</th><th>category_return_2016</th><th>fund_return_2015</th><th>category_return_2015</th><th>fund_return_2014</th><th>category_return_2014</th><th>fund_return_2013</th><th>category_return_2013</th><th>fund_return_2012</th><th>category_return_2012</th><th>fund_return_2011</th><th>category_return_2011</th><th>fund_return_2010</th><th>category_return_2010</th><th>morningstar_risk_rating</th><th>years_up</th><th>years_down</th><th>fund_alpha_3years</th><th>category_alpha_3years</th><th>fund_alpha_5years</th><th>category_alpha_5years</th><th>fund_alpha_10years</th><th>category_alpha_10years</th><th>fund_beta_3years</th><th>category_beta_3years</th><th>fund_beta_5years</th><th>category_beta_5years</th><th>fund_beta_10years</th><th>category_beta_10years</th><th>fund_mean_annual_return_3years</th><th>category_mean_annual_return_3years</th><th>fund_mean_annual_return_5years</th><th>category_mean_annual_return_5years</th><th>fund_mean_annual_return_10years</th><th>category_mean_annual_return_10years</th><th>fund_r_squared_3years</th><th>category_r_squared_3years</th><th>fund_r_squared_5years</th><th>category_r_squared_5years</th><th>fund_r_squared_10years</th><th>category_r_squared_10years</th><th>fund_standard_deviation_3years</th><th>category_standard_deviation_3years</th><th>fund_standard_deviation_5years</th><th>category_standard_deviation_5years</th><th>fund_standard_deviation_10years</th><th>category_standard_deviation_10years</th><th>fund_sharpe_ratio_3years</th><th>category_sharpe_ratio_3years</th><th>fund_sharpe_ratio_5years</th><th>category_sharpe_ratio_5years</th><th>fund_sharpe_ratio_10years</th><th>category_sharpe_ratio_10years</th><th>fund_treynor_ratio_3years</th><th>category_treynor_ratio_3years</th><th>fund_treynor_ratio_5years</th><th>category_treynor_ratio_5years</th><th>fund_treynor_ratio_10years</th><th>category_treynor_ratio_10years</th></tr></thead><tbody><tr><td>PRJPX</td><td>T. Rowe Price Japan Fund</td><td>Japan Stock</td><td>T. Rowe Price</td><td>7.9199E8</td><td>10.76</td><td>0.69</td><td>4</td><td>1991-12-29T00:00:00.000+0000</td><td>Growth</td><td>Large</td><td>USD</td><td>0.95</td><td>1.27</td><td>1.78</td><td>98.22</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>17.32</td><td>1.69</td><td>1.13</td><td>9.06</td><td>10,000.83</td><td>10.62</td><td>16.0</td><td>2.52</td><td>4.31</td><td>9.67</td><td>7.44</td><td>0.0</td><td>14.39</td><td>0.0</td><td>20.36</td><td>14.71</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>10.76</td><td>7.11</td><td>2.39</td><td>0.37</td><td>10.76</td><td>7.11</td><td>-6.97</td><td>-8.15</td><td>12.12</td><td>8.09</td><td>9.71</td><td>7.27</td><td>11.31</td><td>10.3</td><td>-12.17</td><td>-15.07</td><td>32.66</td><td>25.51</td><td>11.2</td><td>2.17</td><td>14.89</td><td>11.97</td><td>-8.52</td><td>-3.94</td><td>29.96</td><td>26.7</td><td>10.69</td><td>11.27</td><td>-8.39</td><td>-13.46</td><td>14.22</td><td>11.94</td><td>3.0</td><td>14.0</td><td>13.0</td><td>4.44</td><td>0.01</td><td>7.4</td><td>0.05</td><td>5.48</td><td>0.05</td><td>0.94</td><td>0.01</td><td>0.77</td><td>0.01</td><td>0.62</td><td>0.01</td><td>1.03</td><td>0.01</td><td>0.84</td><td>0.01</td><td>0.97</td><td>0.01</td><td>58.07</td><td>0.58</td><td>54.31</td><td>0.55</td><td>51.43</td><td>0.44</td><td>12.99</td><td>0.12</td><td>12.56</td><td>0.13</td><td>13.64</td><td>0.14</td><td>0.85</td><td>0.01</td><td>0.74</td><td>0.01</td><td>0.82</td><td>0.01</td><td>11.55</td><td>0.08</td><td>11.54</td><td>0.09</td><td>17.49</td><td>0.17</td></tr><tr><td>PRJQX</td><td>PGIM Jennison Global Opportunities Fund-Class R6</td><td>World Large Stock</td><td>PGIM Funds (Prudential)</td><td>2.03E9</td><td>18.26</td><td>0.0</td><td>5</td><td>2014-12-21T00:00:00.000+0000</td><td>Growth</td><td>Large</td><td>USD</td><td>0.84</td><td>1.11</td><td>0.95</td><td>99.05</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>35.85</td><td>8.47</td><td>5.01</td><td>23.97</td><td>101,888.72</td><td>1.8</td><td>28.69</td><td>9.02</td><td>0.0</td><td>1.37</td><td>20.75</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.16</td><td>31.21</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>18.26</td><td>12.27</td><td>2.35</td><td>1.11</td><td>18.26</td><td>12.27</td><td>9.68</td><td>1.85</td><td>19.33</td><td>9.96</td><td>12.4</td><td>5.89</td><td>0.0</td><td>11.79</td><td>-2.51</td><td>-9.64</td><td>43.47</td><td>23.61</td><td>-4.4</td><td>5.54</td><td>12.8</td><td>-1.69</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5.0</td><td>2.0</td><td>2.0</td><td>9.46</td><td>0.03</td><td>9.63</td><td>0.04</td><td>null</td><td>0.04</td><td>1.16</td><td>0.01</td><td>0.98</td><td>0.01</td><td>null</td><td>0.01</td><td>1.58</td><td>0.01</td><td>1.07</td><td>0.01</td><td>null</td><td>0.01</td><td>65.53</td><td>0.73</td><td>63.15</td><td>0.78</td><td>null</td><td>0.84</td><td>15.14</td><td>0.11</td><td>14.82</td><td>0.11</td><td>null</td><td>0.14</td><td>1.16</td><td>0.01</td><td>0.81</td><td>0.0</td><td>null</td><td>0.01</td><td>15.59</td><td>0.1</td><td>11.84</td><td>0.06</td><td>null</td><td>0.14</td></tr><tr><td>PRJZX</td><td>PGIM Jennison Global Opportunities Fund-Class Z</td><td>World Large Stock</td><td>PGIM Funds (Prudential)</td><td>2.03E9</td><td>18.23</td><td>0.0</td><td>5</td><td>2012-03-13T00:00:00.000+0000</td><td>Growth</td><td>Large</td><td>USD</td><td>0.92</td><td>1.11</td><td>0.95</td><td>99.05</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>35.85</td><td>8.47</td><td>5.01</td><td>23.97</td><td>101,888.72</td><td>1.8</td><td>28.69</td><td>9.02</td><td>0.0</td><td>1.37</td><td>20.75</td><td>0.0</td><td>0.0</td><td>0.0</td><td>7.16</td><td>31.21</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>18.23</td><td>12.27</td><td>2.36</td><td>1.11</td><td>18.23</td><td>12.27</td><td>9.53</td><td>1.85</td><td>19.2</td><td>9.96</td><td>12.29</td><td>5.89</td><td>0.0</td><td>11.79</td><td>-2.61</td><td>-9.64</td><td>43.35</td><td>23.61</td><td>-4.53</td><td>5.54</td><td>12.64</td><td>-1.69</td><td>-1.07</td><td>2.79</td><td>36.21</td><td>25.19</td><td>null</td><td>15.84</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5.0</td><td>3.0</td><td>3.0</td><td>9.34</td><td>0.03</td><td>9.53</td><td>0.04</td><td>null</td><td>0.04</td><td>1.16</td><td>0.01</td><td>0.98</td><td>0.01</td><td>null</td><td>0.01</td><td>1.57</td><td>0.01</td><td>1.06</td><td>0.01</td><td>null</td><td>0.01</td><td>65.64</td><td>0.73</td><td>63.13</td><td>0.78</td><td>null</td><td>0.84</td><td>15.14</td><td>0.11</td><td>14.81</td><td>0.11</td><td>null</td><td>0.14</td><td>1.16</td><td>0.01</td><td>0.81</td><td>0.0</td><td>null</td><td>0.01</td><td>15.47</td><td>0.1</td><td>11.74</td><td>0.06</td><td>null</td><td>0.14</td></tr><tr><td>PRKAX</td><td>PGIM Real Estate Income Fund- Class A</td><td>Real Estate</td><td>PGIM Funds (Prudential)</td><td>1.412E7</td><td>19.77</td><td>4.41</td><td>4</td><td>2015-06-02T00:00:00.000+0000</td><td>Blend</td><td>Medium</td><td>USD</td><td>1.35</td><td>1.22</td><td>0.58</td><td>77.49</td><td>0.0</td><td>2.52</td><td>19.41</td><td>0.0</td><td>23.24</td><td>1.35</td><td>2.61</td><td>2.1</td><td>4,841.58</td><td>0.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>19.77</td><td>16.14</td><td>4.6</td><td>3.49</td><td>19.77</td><td>16.14</td><td>14.85</td><td>17.08</td><td>6.61</td><td>6.0</td><td>0.0</td><td>8.29</td><td>0.0</td><td>17.24</td><td>-10.49</td><td>-5.97</td><td>7.6</td><td>6.22</td><td>9.82</td><td>6.89</td><td>null</td><td>2.41</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2.0</td><td>2.0</td><td>1.0</td><td>-1.13</td><td>-0.01</td><td>null</td><td>0.04</td><td>null</td><td>0.07</td><td>0.74</td><td>0.01</td><td>null</td><td>0.01</td><td>null</td><td>0.01</td><td>0.59</td><td>0.01</td><td>null</td><td>0.01</td><td>null</td><td>0.01</td><td>37.31</td><td>0.3</td><td>null</td><td>0.28</td><td>null</td><td>0.46</td><td>12.2</td><td>0.13</td><td>null</td><td>0.14</td><td>null</td><td>0.18</td><td>0.48</td><td>0.0</td><td>null</td><td>0.01</td><td>null</td><td>0.01</td><td>7.18</td><td>0.07</td><td>null</td><td>0.12</td><td>null</td><td>0.19</td></tr><tr><td>PRKCX</td><td>PGIM Real Estate Income Fund- Class C</td><td>Real Estate</td><td>PGIM Funds (Prudential)</td><td>1.412E7</td><td>19.47</td><td>3.73</td><td>3</td><td>2015-06-02T00:00:00.000+0000</td><td>Blend</td><td>Medium</td><td>USD</td><td>2.1</td><td>1.22</td><td>0.58</td><td>77.49</td><td>0.0</td><td>2.52</td><td>19.41</td><td>0.0</td><td>23.24</td><td>1.35</td><td>2.61</td><td>2.1</td><td>4,841.58</td><td>0.0</td><td>0.0</td><td>0.0</td><td>100.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>3.0</td><td>19.47</td><td>16.14</td><td>4.61</td><td>3.49</td><td>19.47</td><td>16.14</td><td>13.99</td><td>17.08</td><td>5.83</td><td>6.0</td><td>0.0</td><td>8.29</td><td>0.0</td><td>17.24</td><td>-11.06</td><td>-5.97</td><td>6.73</td><td>6.22</td><td>9.13</td><td>6.89</td><td>null</td><td>2.41</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2.0</td><td>2.0</td><td>1.0</td><td>-1.79</td><td>-0.01</td><td>null</td><td>0.04</td><td>null</td><td>0.07</td><td>0.73</td><td>0.01</td><td>null</td><td>0.01</td><td>null</td><td>0.01</td><td>0.53</td><td>0.01</td><td>null</td><td>0.01</td><td>null</td><td>0.01</td><td>36.88</td><td>0.3</td><td>null</td><td>0.28</td><td>null</td><td>0.46</td><td>12.14</td><td>0.13</td><td>null</td><td>0.14</td><td>null</td><td>0.18</td><td>0.42</td><td>0.0</td><td>null</td><td>0.01</td><td>null</td><td>0.01</td><td>6.19</td><td>0.07</td><td>null</td><td>0.12</td><td>null</td><td>0.19</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["## AWS S3\nwork on this section, access management doesn't work so far...\nand most probably the file is not accessed correctly"],"metadata":{}},{"cell_type":"code","source":["sparkETFsDF = spark.read.format('csv').options(header='true', inferSchema='true').load('https://doc0815-myfirstbucket.s3.eu-central-1.amazonaws.com/ETFs.csv')\ndisplay(sparkETFsDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3746941355164773&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>sparkETFsDF <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;csv&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>options<span class=\"ansiyellow\">(</span>header<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;true&apos;</span><span class=\"ansiyellow\">,</span> inferSchema<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&apos;true&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;https://doc0815-myfirstbucket.s3.eu-central-1.amazonaws.com/ETFs.csv&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> display<span class=\"ansiyellow\">(</span>sparkETFsDF<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">load</span><span class=\"ansiblue\">(self, path, format, schema, **options)</span>\n<span class=\"ansigreen\">    164</span>         self<span class=\"ansiyellow\">.</span>options<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">**</span>options<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    165</span>         <span class=\"ansigreen\">if</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> basestring<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 166</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    167</span>         <span class=\"ansigreen\">elif</span> path <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    168</span>             <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">!=</span> list<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o2176.load.\n: java.io.IOException: No FileSystem for scheme: https\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:96)\n\tat org.apache.spark.sql.DataFrameReader.preprocessDeltaLoading(DataFrameReader.scala:226)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:258)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:212)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["%sql\nCREATE TABLE etf USING S3SELECT LOCATION 's3://doc0815-myfirstbucket.s3.eu-central-1.amazonaws.com/ETFs.csv'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [BasicAWSCredentialsProvider: Access key or secret key is null, com.amazonaws.auth.InstanceProfileCredentialsProvider@74191259: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/]\n\tat com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1164)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:762)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:724)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4365)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:5126)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:5100)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4349)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4312)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4306)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketLocation(AmazonS3Client.java:955)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketLocation(AmazonS3Client.java:961)\n\tat com.databricks.s3a.S3AFileSystem.setEndpointByAutoDetectedRegion(S3AFileSystem.java:394)\n\tat com.databricks.s3a.S3AFileSystem.setEndpoint(S3AFileSystem.java:381)\n\tat com.databricks.s3a.S3AFileSystem.initialize(S3AFileSystem.java:274)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:597)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:595)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:595)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:78)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:81)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3424)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3419)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:228)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:85)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:158)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3419)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:707)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:87)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:33)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:33)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:323)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:303)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:303)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:586)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:477)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:544)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:330)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:216)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:122)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:136)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:323)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:303)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:303)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:591)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:586)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:477)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:544)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:330)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:216)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":20},{"cell_type":"markdown","source":["## MongoDB\nConnect to the `impExplorer` database on a MongoDB Atlas cluster, where sensor data is stored in the collection `SensorData`.\n\nRequires:\n- installation of MongoDB driver (must match Spark and Scala versions!)\n- configuration of `spark.mongodb.input.uri` and  `spark.mongodb.output.uri`"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format('com.mongodb.spark.sql.DefaultSource').option('database', 'impExplorer').option('collection', 'SensorData').load()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["# the schema of the dataframe is derived by sampled collection data (MongoDB collections do not have a static schema)\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _id: struct (nullable = true)\n    |-- oid: string (nullable = true)\n-- co2: integer (nullable = true)\n-- humidity: string (nullable = true)\n-- lightlevel: integer (nullable = true)\n-- pressure: string (nullable = true)\n-- sensorID: string (nullable = true)\n-- sensorTimestamp: double (nullable = true)\n-- temperature: string (nullable = true)\n-- voc: integer (nullable = true)\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# show some data for a specific time period\n# the sensorTimestamp is recorded in ms since 01.01.1970, i.e. 1564349723000 is 2019-07-28 21:35:23.000 and 1564350323000 is 2019-07-28 21:45:23.000\ndf.filter( (df['sensorTimestamp'] >= 1564349723000) & (df['sensorTimestamp'] <= 1564350323000) ).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---+---------+----------+---------+----------------+---------------+-----------+---+\n                 _id|co2| humidity|lightlevel| pressure|        sensorID|sensorTimestamp|temperature|voc|\n+--------------------+---+---------+----------+---------+----------------+---------------+-----------+---+\n[5d3e1aa125e97a67...|558|64.911972|     12339|  947.948|2391d553094457ee| 1.564349723E12|   27.48966|338|\n[5d3e1aa125e97a67...|569|64.820419|     11714|948.07959|2391d553094457ee| 1.564349784E12|  27.471012|345|\n[5d3e1aa125e97a67...|570| 64.84507|     12339|948.03296|2391d553094457ee| 1.564349845E12|  27.545605|333|\n[5d3e1aa125e97a67...|568|64.823944|     12323|948.09106|2391d553094457ee| 1.564349906E12|  27.508308|337|\n[5d3e1aa125e97a67...|580|64.757042|     12339|948.06274|2391d553094457ee| 1.564349967E12|  27.545605|349|\n[5d3e1aa125e97a67...|567|    64.75|     12339|948.05298|2391d553094457ee| 1.564350028E12|  27.508308|345|\n[5d3e1aa125e97a67...|543|64.757042|     12323|948.08643|2391d553094457ee| 1.564350089E12|  27.471012|340|\n[5d3e1aa125e97a67...|569|64.704224|     12339|948.11597|2391d553094457ee|  1.56435015E12|  27.526957|339|\n[5d3e1aa125e97a67...|579| 64.69014|     12339|948.10498|2391d553094457ee| 1.564350211E12|  27.508308|349|\n[5d3e1aa125e97a67...|571|64.679581|     12339|948.15137|2391d553094457ee| 1.564350272E12|  27.526957|352|\n+--------------------+---+---------+----------+---------+----------------+---------------+-----------+---+\n\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["# Delta Lake\nExplore what it is..."],"metadata":{}}],"metadata":{"name":"accessing_data","notebookId":3924776977511454},"nbformat":4,"nbformat_minor":0}
